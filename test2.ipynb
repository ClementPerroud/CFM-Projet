{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-15 16:07:31.875634: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-15 16:07:39.288433: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clement/miniconda3/envs/tf/lib/\n",
      "2023-03-15 16:07:39.288997: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clement/miniconda3/envs/tf/lib/\n",
      "2023-03-15 16:07:39.289013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from AdvASLTM import *\n",
    "from random import shuffle\n",
    "\n",
    "SEQ_LEN = 20\n",
    "train_ratio = 0.7\n",
    "PREDICTON = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"BTC-Hourly.csv\", parse_dates=[\"date\"], index_col= \"date\")\n",
    "\n",
    "df.sort_index(inplace = True)\n",
    "\n",
    "# Create label\n",
    "df[\"label\"] = (df[\"close\"] < df[\"close\"].shift(-PREDICTON))*1\n",
    "\n",
    "df[\"high\"] = df[\"high\"] / df[\"close\"]\n",
    "df[\"low\"] = df[\"low\"] / df[\"close\"]\n",
    "df[\"open\"] = df[\"open\"] / df[\"close\"]\n",
    "df[\"close\"] = df[\"close\"].pct_change()\n",
    "df[\"volume\"] = df[\"Volume BTC\"] / df[\"Volume BTC\"].rolling(48).median()\n",
    "features_columns = [\"open\", \"high\", \"low\", \"close\", \"volume\"]\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "scaler = RobustScaler().fit(df[features_columns])\n",
    "df[features_columns] = scaler.transform(df[features_columns])\n",
    "\n",
    "\n",
    "#Generate Sequences\n",
    "sequences = []\n",
    "array_features = df[features_columns].to_numpy()\n",
    "array_label = df[\"label\"].to_numpy()\n",
    "for i in range(SEQ_LEN, len(df)):\n",
    "    sequences.append([\n",
    "        array_features[i - SEQ_LEN : i],\n",
    "        array_label[i-1]\n",
    "    ])\n",
    "del array_features, array_label\n",
    "\n",
    "#shuffle(sequences)\n",
    "X, y = zip(*sequences)\n",
    "\n",
    "\n",
    "train_limit = int(train_ratio * len(df))\n",
    "\n",
    "X_train = np.array(X[:train_limit])\n",
    "y_train = np.array(y[:train_limit])\n",
    "\n",
    "X_validation = np.array(X[train_limit:])\n",
    "y_validation = np.array(y[train_limit:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ALSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "182/182 [==============================] - 12s 33ms/step - loss: 0.6943 - acc: 0.5013 - val_loss: 0.6924 - val_acc: 0.5149\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6927 - acc: 0.5187 - val_loss: 0.6920 - val_acc: 0.5110\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6925 - acc: 0.5171 - val_loss: 0.6919 - val_acc: 0.5175\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6921 - acc: 0.5230 - val_loss: 0.6915 - val_acc: 0.5235\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6917 - acc: 0.5243 - val_loss: 0.6910 - val_acc: 0.5329\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6912 - acc: 0.5257 - val_loss: 0.6907 - val_acc: 0.5329\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6911 - acc: 0.5297 - val_loss: 0.6908 - val_acc: 0.5313\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6903 - acc: 0.5302 - val_loss: 0.6905 - val_acc: 0.5378\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6899 - acc: 0.5337 - val_loss: 0.6906 - val_acc: 0.5389\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6901 - acc: 0.5347 - val_loss: 0.6906 - val_acc: 0.5388\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6890 - acc: 0.5428 - val_loss: 0.6908 - val_acc: 0.5400\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6895 - acc: 0.5397 - val_loss: 0.6908 - val_acc: 0.5409\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6898 - acc: 0.5361 - val_loss: 0.6909 - val_acc: 0.5383\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 5s 29ms/step - loss: 0.6886 - acc: 0.5419 - val_loss: 0.6909 - val_acc: 0.5354\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6893 - acc: 0.5366 - val_loss: 0.6909 - val_acc: 0.5367\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 5s 29ms/step - loss: 0.6882 - acc: 0.5445 - val_loss: 0.6910 - val_acc: 0.5370\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6883 - acc: 0.5452 - val_loss: 0.6911 - val_acc: 0.5355\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 6s 30ms/step - loss: 0.6892 - acc: 0.5406 - val_loss: 0.6912 - val_acc: 0.5331\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 6s 33ms/step - loss: 0.6870 - acc: 0.5463 - val_loss: 0.6915 - val_acc: 0.5365\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 6s 32ms/step - loss: 0.6872 - acc: 0.5449 - val_loss: 0.6917 - val_acc: 0.5359\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 7s 37ms/step - loss: 0.6884 - acc: 0.5440 - val_loss: 0.6918 - val_acc: 0.5337\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 6s 31ms/step - loss: 0.6880 - acc: 0.5429 - val_loss: 0.6917 - val_acc: 0.5353\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 5s 29ms/step - loss: 0.6873 - acc: 0.5452 - val_loss: 0.6921 - val_acc: 0.5341\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 6s 30ms/step - loss: 0.6874 - acc: 0.5459 - val_loss: 0.6924 - val_acc: 0.5342\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 6s 32ms/step - loss: 0.6871 - acc: 0.5465 - val_loss: 0.6924 - val_acc: 0.5330\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 5s 30ms/step - loss: 0.6878 - acc: 0.5443 - val_loss: 0.6922 - val_acc: 0.5320\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 5s 28ms/step - loss: 0.6872 - acc: 0.5453 - val_loss: 0.6922 - val_acc: 0.5339\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 5s 27ms/step - loss: 0.6874 - acc: 0.5427 - val_loss: 0.6923 - val_acc: 0.5318\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 6s 30ms/step - loss: 0.6867 - acc: 0.5495 - val_loss: 0.6920 - val_acc: 0.5326\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 6s 31ms/step - loss: 0.6867 - acc: 0.5443 - val_loss: 0.6923 - val_acc: 0.5323\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 6s 34ms/step - loss: 0.6869 - acc: 0.5501 - val_loss: 0.6921 - val_acc: 0.5336\n",
      "Epoch 32/100\n",
      "  4/182 [..............................] - ETA: 4s - loss: 0.6815 - acc: 0.5430"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[130], line 22\u001b[0m\n\u001b[1;32m     15\u001b[0m lstm_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mModel(inputs \u001b[39m=\u001b[39m inputs, outputs \u001b[39m=\u001b[39m output)\n\u001b[1;32m     16\u001b[0m lstm_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     17\u001b[0m     optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate \u001b[39m=\u001b[39m \u001b[39m1E-4\u001b[39m),\n\u001b[1;32m     18\u001b[0m     loss\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     19\u001b[0m     metrics \u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m )\n\u001b[0;32m---> 22\u001b[0m lstm_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     23\u001b[0m     X_train, y_train,\n\u001b[1;32m     24\u001b[0m     validation_data\u001b[39m=\u001b[39;49m (X_validation, y_validation),\n\u001b[1;32m     25\u001b[0m     epochs\u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     26\u001b[0m     batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m   \n\u001b[1;32m     27\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:133\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[39;00m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[0;32m--> 133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_maybe_define_function(args, kwargs)\n\u001b[1;32m    134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39mconcrete_function\u001b[39m.\u001b[39mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:336\u001b[0m, in \u001b[0;36mTracingCompiler._maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    331\u001b[0m captures \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_captures_container\u001b[39m.\u001b[39mget_snapshot()\n\u001b[1;32m    333\u001b[0m \u001b[39m# cache_key_deletion_observer is useless here. It's based on all captures.\u001b[39;00m\n\u001b[1;32m    334\u001b[0m \u001b[39m# A new cache key will be built later when saving ConcreteFunction because\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m# only active captures should be saved.\u001b[39;00m\n\u001b[0;32m--> 336\u001b[0m lookup_func_key, _ \u001b[39m=\u001b[39m function_context\u001b[39m.\u001b[39;49mmake_cache_key((args, kwargs),\n\u001b[1;32m    337\u001b[0m                                                      captures)\n\u001b[1;32m    338\u001b[0m concrete_function \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_function_cache\u001b[39m.\u001b[39mlookup(lookup_func_key, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    339\u001b[0m \u001b[39mif\u001b[39;00m concrete_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/function_context.py:133\u001b[0m, in \u001b[0;36mmake_cache_key\u001b[0;34m(args, captures)\u001b[0m\n\u001b[1;32m    131\u001b[0m   captures \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m()\n\u001b[1;32m    132\u001b[0m signature_context \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mInternalTracingContext()\n\u001b[0;32m--> 133\u001b[0m args_signature \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39;49mfrom_value(\n\u001b[1;32m    134\u001b[0m     args, signature_context)\n\u001b[1;32m    135\u001b[0m captures_dict_tracetype \u001b[39m=\u001b[39m trace_type\u001b[39m.\u001b[39mfrom_value(\n\u001b[1;32m    136\u001b[0m     captures, signature_context)\n\u001b[1;32m    138\u001b[0m \u001b[39m# TODO(fmuham): Use the actual FunctionType\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:129\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mNamedTuple\u001b[39m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    127\u001b[0m         named_tuple_type, \u001b[39mtuple\u001b[39m(from_value(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m value))\n\u001b[1;32m    128\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39;49mTuple(\u001b[39m*\u001b[39;49m(from_value(c, context) \u001b[39mfor\u001b[39;49;00m c \u001b[39min\u001b[39;49;00m value))\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    132\u001b[0m   \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mDict({k: from_value(value[k], context) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m value})\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:129\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mNamedTuple\u001b[39m.\u001b[39mfrom_type_and_attributes(\n\u001b[1;32m    127\u001b[0m         named_tuple_type, \u001b[39mtuple\u001b[39m(from_value(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m value))\n\u001b[1;32m    128\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mTuple(\u001b[39m*\u001b[39m(from_value(c, context) \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m value))\n\u001b[1;32m    131\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, collections\u001b[39m.\u001b[39mabc\u001b[39m.\u001b[39mMapping):\n\u001b[1;32m    132\u001b[0m   \u001b[39mreturn\u001b[39;00m default_types\u001b[39m.\u001b[39mDict({k: from_value(value[k], context) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m value})\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/core/function/trace_type/trace_type_builder.py:114\u001b[0m, in \u001b[0;36mfrom_value\u001b[0;34m(value, context)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m context\u001b[39m.\u001b[39mis_legacy_signature \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(value, trace\u001b[39m.\u001b[39mTraceType):\n\u001b[1;32m    113\u001b[0m   \u001b[39mreturn\u001b[39;00m value\n\u001b[0;32m--> 114\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(value, trace\u001b[39m.\u001b[39;49mSupportsTracingProtocol):\n\u001b[1;32m    115\u001b[0m   \u001b[39mreturn\u001b[39;00m value\u001b[39m.\u001b[39m__tf_tracing_type__(context)\n\u001b[1;32m    117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39m__wrapped__\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/typing.py:1145\u001b[0m, in \u001b[0;36m_ProtocolMeta.__instancecheck__\u001b[0;34m(cls, instance)\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__instancecheck__\u001b[39m(\u001b[39mcls\u001b[39m, instance):\n\u001b[1;32m   1142\u001b[0m     \u001b[39m# We need this method for situations where attributes are\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m     \u001b[39m# assigned in __init__.\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m     \u001b[39mif\u001b[39;00m ((\u001b[39mnot\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m_is_protocol\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m-> 1145\u001b[0m             _is_callable_members_only(\u001b[39mcls\u001b[39;49m)) \u001b[39mand\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m             \u001b[39missubclass\u001b[39m(instance\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m, \u001b[39mcls\u001b[39m)):\n\u001b[1;32m   1147\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39m_is_protocol:\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/typing.py:1084\u001b[0m, in \u001b[0;36m_is_callable_members_only\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_callable_members_only\u001b[39m(\u001b[39mcls\u001b[39m):\n\u001b[1;32m   1083\u001b[0m     \u001b[39m# PEP 544 prohibits using issubclass() with protocols that have non-method members.\u001b[39;00m\n\u001b[0;32m-> 1084\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mall\u001b[39m(callable(\u001b[39mgetattr\u001b[39m(\u001b[39mcls\u001b[39m, attr, \u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m _get_protocol_attrs(\u001b[39mcls\u001b[39;49m))\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/typing.py:1077\u001b[0m, in \u001b[0;36m_get_protocol_attrs\u001b[0;34m(cls)\u001b[0m\n\u001b[1;32m   1075\u001b[0m     annotations \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(base, \u001b[39m'\u001b[39m\u001b[39m__annotations__\u001b[39m\u001b[39m'\u001b[39m, {})\n\u001b[1;32m   1076\u001b[0m     \u001b[39mfor\u001b[39;00m attr \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m(base\u001b[39m.\u001b[39m\u001b[39m__dict__\u001b[39m\u001b[39m.\u001b[39mkeys()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(annotations\u001b[39m.\u001b[39mkeys()):\n\u001b[0;32m-> 1077\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m attr\u001b[39m.\u001b[39;49mstartswith(\u001b[39m'\u001b[39;49m\u001b[39m_abc_\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mand\u001b[39;00m attr \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m EXCLUDED_ATTRIBUTES:\n\u001b[1;32m   1078\u001b[0m             attrs\u001b[39m.\u001b[39madd(attr)\n\u001b[1;32m   1079\u001b[0m \u001b[39mreturn\u001b[39;00m attrs\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "inputs = tf.keras.layers.Input(shape=X_train.shape[1:])\n",
    "x = tf.keras.layers.Dense(32, activation = \"tanh\", kernel_regularizer = tf.keras.regularizers.L2(0.01))(inputs )\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "h = tf.keras.layers.LSTM(64, return_sequences= True, dropout= 0.4)(x)\n",
    "alpha = TemporalAttentionLayer(units = 64)(h)\n",
    "x = LatentLayer()([alpha,h])\n",
    "x = tf.keras.layers.Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "x = tf.keras.layers.Dense(32, activation = \"relu\", kernel_regularizer = tf.keras.regularizers.L2(0.01))(x)\n",
    "x = tf.keras.layers.Dropout(0.3)(x)\n",
    "output = tf.keras.layers.Dense(2, activation = \"softmax\")(x)\n",
    "\n",
    "\n",
    "\n",
    "lstm_model = tf.keras.models.Model(inputs = inputs, outputs = output)\n",
    "lstm_model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1E-4),\n",
    "    loss= \"sparse_categorical_crossentropy\",\n",
    "    metrics =[\"acc\"]\n",
    ")\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data= (X_validation, y_validation),\n",
    "    epochs= 100,\n",
    "    batch_size = 128   \n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "182/182 [==============================] - 10s 25ms/step - loss: 0.6908 - acc: 0.5305 - val_loss: 0.6907 - val_acc: 0.5341\n",
      "Epoch 2/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6883 - acc: 0.5432 - val_loss: 0.6902 - val_acc: 0.5376\n",
      "Epoch 3/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6878 - acc: 0.5456 - val_loss: 0.6921 - val_acc: 0.5333\n",
      "Epoch 4/100\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6883 - acc: 0.5406 - val_loss: 0.6901 - val_acc: 0.5315\n",
      "Epoch 5/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6860 - acc: 0.5481 - val_loss: 0.6972 - val_acc: 0.5324\n",
      "Epoch 6/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6874 - acc: 0.5440 - val_loss: 0.6912 - val_acc: 0.5388\n",
      "Epoch 7/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6868 - acc: 0.5448 - val_loss: 0.7008 - val_acc: 0.5317\n",
      "Epoch 8/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6854 - acc: 0.5488 - val_loss: 0.6951 - val_acc: 0.5321\n",
      "Epoch 9/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6858 - acc: 0.5503 - val_loss: 0.6954 - val_acc: 0.5335\n",
      "Epoch 10/100\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6855 - acc: 0.5539 - val_loss: 0.6951 - val_acc: 0.5407\n",
      "Epoch 11/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6844 - acc: 0.5534 - val_loss: 0.6943 - val_acc: 0.5360\n",
      "Epoch 12/100\n",
      "182/182 [==============================] - 4s 20ms/step - loss: 0.6840 - acc: 0.5554 - val_loss: 0.6943 - val_acc: 0.5315\n",
      "Epoch 13/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6839 - acc: 0.5529 - val_loss: 0.6980 - val_acc: 0.5256\n",
      "Epoch 14/100\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6818 - acc: 0.5628 - val_loss: 0.6955 - val_acc: 0.5267\n",
      "Epoch 15/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6810 - acc: 0.5647 - val_loss: 0.6983 - val_acc: 0.5310\n",
      "Epoch 16/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6814 - acc: 0.5600 - val_loss: 0.6938 - val_acc: 0.5336\n",
      "Epoch 17/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6817 - acc: 0.5610 - val_loss: 0.7033 - val_acc: 0.5331\n",
      "Epoch 18/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6805 - acc: 0.5672 - val_loss: 0.6957 - val_acc: 0.5340\n",
      "Epoch 19/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6803 - acc: 0.5657 - val_loss: 0.6964 - val_acc: 0.5351\n",
      "Epoch 20/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6803 - acc: 0.5651 - val_loss: 0.6998 - val_acc: 0.5301\n",
      "Epoch 21/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6793 - acc: 0.5675 - val_loss: 0.7018 - val_acc: 0.5244\n",
      "Epoch 22/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6785 - acc: 0.5693 - val_loss: 0.6964 - val_acc: 0.5344\n",
      "Epoch 23/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6769 - acc: 0.5724 - val_loss: 0.7013 - val_acc: 0.5301\n",
      "Epoch 24/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6780 - acc: 0.5693 - val_loss: 0.6980 - val_acc: 0.5310\n",
      "Epoch 25/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6770 - acc: 0.5764 - val_loss: 0.7105 - val_acc: 0.5290\n",
      "Epoch 26/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6767 - acc: 0.5761 - val_loss: 0.7041 - val_acc: 0.5309\n",
      "Epoch 27/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6762 - acc: 0.5703 - val_loss: 0.7099 - val_acc: 0.5270\n",
      "Epoch 28/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6772 - acc: 0.5725 - val_loss: 0.6983 - val_acc: 0.5309\n",
      "Epoch 29/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6768 - acc: 0.5739 - val_loss: 0.7028 - val_acc: 0.5359\n",
      "Epoch 30/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6749 - acc: 0.5774 - val_loss: 0.6976 - val_acc: 0.5235\n",
      "Epoch 31/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6750 - acc: 0.5704 - val_loss: 0.7010 - val_acc: 0.5304\n",
      "Epoch 32/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6749 - acc: 0.5796 - val_loss: 0.7037 - val_acc: 0.5314\n",
      "Epoch 33/100\n",
      "182/182 [==============================] - 3s 17ms/step - loss: 0.6741 - acc: 0.5777 - val_loss: 0.6965 - val_acc: 0.5333\n",
      "Epoch 34/100\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6745 - acc: 0.5751 - val_loss: 0.6987 - val_acc: 0.5362\n",
      "Epoch 35/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6730 - acc: 0.5792 - val_loss: 0.7006 - val_acc: 0.5267\n",
      "Epoch 36/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6748 - acc: 0.5753 - val_loss: 0.7048 - val_acc: 0.5286\n",
      "Epoch 37/100\n",
      "182/182 [==============================] - 3s 18ms/step - loss: 0.6732 - acc: 0.5774 - val_loss: 0.7048 - val_acc: 0.5339\n",
      "Epoch 38/100\n",
      "182/182 [==============================] - 3s 16ms/step - loss: 0.6732 - acc: 0.5768 - val_loss: 0.7034 - val_acc: 0.5315\n",
      "Epoch 39/100\n",
      " 79/182 [============>.................] - ETA: 1s - loss: 0.6724 - acc: 0.5827"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[120], line 16\u001b[0m\n\u001b[1;32m      1\u001b[0m lstm_model \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mmodels\u001b[39m.\u001b[39mSequential([\n\u001b[1;32m      2\u001b[0m     \u001b[39m# tf.keras.layers.LSTM(64, return_sequences= True, dropout= 0.4),\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     \u001b[39m# tf.keras.layers.LSTM(64, return_sequences= True, dropout= 0.4),\u001b[39;00m\n\u001b[1;32m      4\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mLSTM(\u001b[39m64\u001b[39m,dropout\u001b[39m=\u001b[39m \u001b[39m0.4\u001b[39m, return_sequences\u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m),\n\u001b[1;32m      5\u001b[0m     tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlayers\u001b[39m.\u001b[39mDense(\u001b[39m2\u001b[39m, activation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msoftmax\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m ])\n\u001b[1;32m      9\u001b[0m lstm_model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m     10\u001b[0m     optimizer \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mAdam(learning_rate \u001b[39m=\u001b[39m \u001b[39m1E-2\u001b[39m),\n\u001b[1;32m     11\u001b[0m     loss\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     metrics \u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39macc\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     13\u001b[0m )\n\u001b[0;32m---> 16\u001b[0m lstm_model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     17\u001b[0m     X_train, y_train,\n\u001b[1;32m     18\u001b[0m     validation_data\u001b[39m=\u001b[39;49m (X_validation, y_validation),\n\u001b[1;32m     19\u001b[0m     epochs\u001b[39m=\u001b[39;49m \u001b[39m100\u001b[39;49m,\n\u001b[1;32m     20\u001b[0m     batch_size \u001b[39m=\u001b[39;49m \u001b[39m128\u001b[39;49m,\n\u001b[1;32m     21\u001b[0m     \n\u001b[1;32m     22\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     66\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[39mwith\u001b[39;00m tf\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mexperimental\u001b[39m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[39m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[39m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_function(iterator)\n\u001b[1;32m   1651\u001b[0m     \u001b[39mif\u001b[39;00m data_handler\u001b[39m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[39m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[39m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[39m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_no_variable_creation_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)  \u001b[39m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_variable_creation_fn \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[39m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[39m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[39mreturn\u001b[39;00m concrete_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[1;32m    135\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mconcrete_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1746\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    379\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    380\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    381\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    382\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    383\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    384\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # tf.keras.layers.LSTM(64, return_sequences= True, dropout= 0.4),\n",
    "    # tf.keras.layers.LSTM(64, return_sequences= True, dropout= 0.4),\n",
    "    tf.keras.layers.LSTM(64,dropout= 0.4, return_sequences= False),\n",
    "    tf.keras.layers.Dense(2, activation = \"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1E-3),\n",
    "    loss= \"sparse_categorical_crossentropy\",\n",
    "    metrics =[\"acc\"]\n",
    ")\n",
    "\n",
    "\n",
    "lstm_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data= (X_validation, y_validation),\n",
    "    epochs= 100,\n",
    "    batch_size = 128,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------x-----------------------------------------------\n",
      "Diff mean : 1.6196834621950984e-05\n",
      "Diff std : 0.0020502840634435415\n",
      "---------------------------------------------h-----------------------------------------------\n",
      "Diff mean : -1.8047420553557458e-06\n",
      "Diff std : 0.0004248569021001458\n",
      "---------------------------------------------alpha-----------------------------------------------\n",
      "Diff mean : -1.6393514670198783e-06\n",
      "Diff std : 0.0002155575784854591\n",
      "---------------------------------------------e-----------------------------------------------\n",
      "Diff mean : -2.4673454390722327e-06\n",
      "Diff std : 0.00017854946781881154\n",
      "---------------------------------------------y_pred-----------------------------------------------\n",
      "Diff mean : 6.386637687683105e-05\n",
      "Diff std : 0.0\n"
     ]
    }
   ],
   "source": [
    "index1, index2 = 100, 200\n",
    "results1 = model(X_validation[index1:index1 + 1], debug = True, training = False)\n",
    "results2 = model(X_validation[index2:index2 + 1], debug = True, training = False)\n",
    "for key in results1.keys():\n",
    "    print(f\"---------------------------------------------{key}-----------------------------------------------\")\n",
    "    print(f\"Diff mean : {tf.reduce_mean(results1[key] - results2[key])}\")\n",
    "    print(f\"Diff std : {tf.math.reduce_std(results1[key] - results2[key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"adv_alstm_model_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_8 (Dense)             multiple                  80        \n",
      "                                                                 \n",
      " lstm_8 (LSTM)               multiple                  2112      \n",
      "                                                                 \n",
      " temporal_attention_layer_8   multiple                 288       \n",
      " (TemporalAttentionLayer)                                        \n",
      "                                                                 \n",
      " latent_layer_8 (LatentLayer  multiple                 0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " prediction_layer_8 (Predict  multiple                 33        \n",
      " ionLayer)                                                       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,513\n",
      "Trainable params: 2,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
