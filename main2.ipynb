{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T14:59:13.695522Z",
     "iopub.status.busy": "2023-03-14T14:59:13.694498Z",
     "iopub.status.idle": "2023-03-14T14:59:13.703264Z",
     "shell.execute_reply": "2023-03-14T14:59:13.702057Z",
     "shell.execute_reply.started": "2023-03-14T14:59:13.695475Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-16 09:33:13.953588: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-16 09:33:16.687260: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clement/miniconda3/envs/tf/lib/\n",
      "2023-03-16 09:33:16.687488: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/clement/miniconda3/envs/tf/lib/\n",
      "2023-03-16 09:33:16.687500: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from utils import *\n",
    "from layers import *\n",
    "from AdvASLTM import *\n",
    "import keras_tuner\n",
    "from sklearn.preprocessing import RobustScaler, QuantileTransformer, PowerTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T14:59:13.704466Z",
     "iopub.status.busy": "2023-03-14T14:59:13.704193Z",
     "iopub.status.idle": "2023-03-14T14:59:28.638245Z",
     "shell.execute_reply": "2023-03-14T14:59:28.637397Z",
     "shell.execute_reply.started": "2023-03-14T14:59:13.704435Z"
    }
   },
   "outputs": [],
   "source": [
    "nrows = None\n",
    "seq_len = 30\n",
    "train_ratio = 0.7\n",
    "\n",
    "x_df = pd.read_csv(\"input_training.csv\", index_col=\"ID\", nrows= nrows)\n",
    "y_df = pd.read_csv(\"output_training_gmEd6Zt.csv\", index_col=\"ID\", nrows= nrows)\n",
    "x_test_df = pd.read_csv(\"input_test.csv\", index_col=\"ID\")\n",
    "\n",
    "x_df.sort_values(by=\"day\", inplace= True)\n",
    "x_test_df.sort_values(by=\"day\", inplace= True)\n",
    "\n",
    "y_df[\"reod\"] = y_df[\"reod\"] + 1\n",
    "\n",
    "\n",
    "train_limit = int(train_ratio*len(x_df))\n",
    "\n",
    "train_df = x_df.iloc[:train_limit].join(y_df.iloc[:train_limit]).copy(deep=True)\n",
    "validation_df = x_df.iloc[train_limit:].join(y_df.iloc[train_limit:]).copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_filter = re.compile(\"r[0-9]+\")\n",
    "features_columns = list(filter(r_filter.match, train_df.columns))[-seq_len:]\n",
    "preprocessed_features_columns = [f\"preprocessed_{col}\" for col in features_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deal with NaN\n",
    "train_df.replace(np.nan, 0, inplace = True)\n",
    "validation_df.replace(np.nan, 0, inplace = True)\n",
    "x_test_df.replace(np.nan, 0, inplace = True)\n",
    "\n",
    "# Scale\n",
    "scaler = RobustScaler(unit_variance = True, with_centering= True).fit(train_df[features_columns])\n",
    "train_df[preprocessed_features_columns] = scaler.transform(train_df[features_columns])\n",
    "validation_df[preprocessed_features_columns] = scaler.transform(validation_df[features_columns])\n",
    "x_test_df[preprocessed_features_columns] = scaler.transform(x_test_df[features_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T14:59:30.532784Z",
     "iopub.status.busy": "2023-03-14T14:59:30.532534Z",
     "iopub.status.idle": "2023-03-14T14:59:30.857399Z",
     "shell.execute_reply": "2023-03-14T14:59:30.855586Z",
     "shell.execute_reply.started": "2023-03-14T14:59:30.532760Z"
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Generate sequences\n",
    "def generate_sequences(df, nb_sequences, nb_stocks, only_x = False):\n",
    "\n",
    "    if not only_x: df_group_by_day = df[[\"day\", \"reod\"] + preprocessed_features_columns].groupby(\"day\").apply(lambda x : np.array(x))\n",
    "    else:df_group_by_day = df[[\"day\"] + preprocessed_features_columns].groupby(\"day\").apply(lambda x : np.array(x))\n",
    "\n",
    "    #df_group_by_day_y = df[[\"day\",\"reod\"]].groupby(\"day\").apply(lambda x : np.array(x))\n",
    "    sequences = []\n",
    "    for i in range(nb_sequences):\n",
    "        iday = i % len(df_group_by_day)\n",
    "        try:\n",
    "            picked_stocks = np.random.choice(df_group_by_day.iat[iday].shape[0], size = nb_stocks, replace = False)\n",
    "            if not only_x:\n",
    "                sequences.append([\n",
    "                    df_group_by_day.iat[iday][picked_stocks, 2:],\n",
    "                    df_group_by_day.iat[iday][picked_stocks, 1]\n",
    "                ])\n",
    "            else:sequences.append(\n",
    "                    df_group_by_day.iat[iday][picked_stocks, 1:],)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if not only_x:\n",
    "        X, y = zip(*sequences)\n",
    "        return np.array(X)[..., np.newaxis], np.array(y)\n",
    "    return np.array(sequences)[..., np.newaxis]\n",
    "\n",
    "X_train, y_train = generate_sequences(train_df, 50000, 10)\n",
    "X_validation, y_validation = generate_sequences(validation_df, 30000, 10)\n",
    "\n",
    "X_test = generate_sequences(x_test_df, 50000, 10, only_x= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((50000, 10, 30, 1), (50000, 10))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout = 0.3\n",
    "\n",
    "inputs = tf.keras.layers.Input(X_train.shape[1:])\n",
    "\n",
    "lstm_layer = tf.keras.layers.LSTM(64, dropout = dropout, return_sequences= False)\n",
    "distributed_lstm_layer = tf.keras.layers.TimeDistributed(lstm_layer)\n",
    "flatten_layer = tf.keras.layers.Flatten()\n",
    "\n",
    "global_dense1 = tf.keras.layers.Dense(128, activation= \"relu\")\n",
    "global_dense2 = tf.keras.layers.Dense(64, activation= \"relu\")\n",
    "global_dense3 = tf.keras.layers.Dense(32, activation= \"relu\")\n",
    "global_dim_reshaper = tf.keras.layers.Lambda(lambda x : tf.tile(tf.expand_dims(x, axis = 1), multiples= [1 ,X_train.shape[1], 1]))\n",
    "\n",
    "local_dense1 = tf.keras.layers.Dense(32, activation= \"relu\")\n",
    "\n",
    "concatenate_layer = tf.keras.layers.Concatenate(axis=-1)\n",
    "dense_output = tf.keras.layers.Dense(3, activation= \"softmax\")\n",
    "\n",
    "\n",
    "\n",
    "h_s = distributed_lstm_layer(inputs)\n",
    "\n",
    "x = flatten_layer(h_s)\n",
    "x = global_dense1(x)\n",
    "x = global_dense2(x)\n",
    "x = global_dense3(x)\n",
    "global_latent_space = global_dim_reshaper(x)\n",
    "\n",
    "local_latent_spaces = local_dense1(h_s)\n",
    "\n",
    "combined_latent_spaces = concatenate_layer([local_latent_spaces, global_latent_space])\n",
    "dense_output = dense_output(combined_latent_spaces)\n",
    "\n",
    "\n",
    "model = tf.keras.models.Model(inputs = inputs, outputs = dense_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-14T16:45:34.132556Z",
     "iopub.status.busy": "2023-03-14T16:45:34.132172Z",
     "iopub.status.idle": "2023-03-14T16:46:43.431643Z",
     "shell.execute_reply": "2023-03-14T16:46:43.430398Z",
     "shell.execute_reply.started": "2023-03-14T16:45:34.132528Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 22s 21ms/step - loss: 1.0240 - acc: 0.4835 - sparse_categorical_accuracy: 0.4835 - val_loss: 0.9148 - val_acc: 0.6301 - val_sparse_categorical_accuracy: 0.6301\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 17s 21ms/step - loss: 1.0150 - acc: 0.4904 - sparse_categorical_accuracy: 0.4904 - val_loss: 0.8830 - val_acc: 0.6855 - val_sparse_categorical_accuracy: 0.6855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7efabfecbb50>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate = 1E-3),\n",
    "    loss= \"sparse_categorical_crossentropy\",\n",
    "    metrics =[\"sparse_categorical_accuracy\"]\n",
    ")\n",
    "\n",
    "\n",
    "# model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "#     filepath=\"model_weights/Model_{epoch:02d}-{val_loss:.4f}-{val_acc:0.4f}.hdf5\",\n",
    "#     save_weights_only=False,\n",
    "#     monitor='val_acc',\n",
    "#     mode='max',\n",
    "#     save_best_only=True)\n",
    "\n",
    "# early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "#     monitor=\"val_loss\",\n",
    "#     patience=12,\n",
    "#     verbose=0,\n",
    "#     mode=\"auto\",\n",
    "#     baseline=None,\n",
    "#     restore_best_weights=True,\n",
    "#     start_from_epoch=0,\n",
    "# )\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data = (X_validation, y_validation),\n",
    "    batch_size= 64,\n",
    "    epochs = 2,\n",
    "    #callbacks=[model_checkpoint_callback, early_stop]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 9ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_validation)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int64, numpy=array([0, 0, 0, 0, 0, 0, 0, 1, 1, 1])>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(y_pred[2000], axis = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 2., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_validation[2002]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
